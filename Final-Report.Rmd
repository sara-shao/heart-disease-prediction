---
title: "Final-Report"
date: "12/13/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning= FALSE,
                      message = FALSE,
                     fig.align = "center",
                     fig.width=5, 
                     fig.height=3)
```

## I. Introduction

  Cardiovascular diseases (CVDs) are a group of disorders of the heart and blood vessels. CVDs include a range of conditions that include blood vessel disease, such as coronary artery disease; heart rhythm problems (arrhythmias); heart defects at birth (congenital heart defects); heart valve disease; disease of the heart muscle; heart infections, and many more. Although many forms of CVD can be prevented or treated with healthy lifestyle choices, some can not. CVDs are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 32% of all global deaths. Over 85% of deaths from CVD were due to heart attack and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Cardiovascular diseases (CVDs) are the number one cause of death globally. 
  
  Currently, there are several different ways for physicians to diagnose patients that they believe to be at risk for Cardiovascular Diseases (CVDs). The practices vary by country, but often include the physician checking the patient’s blood pressure, cholesterol level, and conducting further tests such as exercise stress tests, X-rays, etc. Currently, there are many issues with the current diagnostic methods. A [study] (https://www.sciencedaily.com/releases/2009/11/091116103435.htm) of 500 patients found a false positive reading between 77 and 82 percent in patients in patients at risk of CVD screened by ECG, and a false negative reading between 6 to 7 percent in the same patient population . People with CVDs or who are at high risk of CVDs need early detection and management wherein a machine learning model can be of great help.
  
  Using our Cardiovascular Heart Disease data, we have two main goals. Our first goal is to create models for the purpose of prediction. These can be used to assess the likelihood of a heart disease diagnostic for potential at-risk patients based on a number of factors such as age of the and sex of the patient, blood pressure, cholesterol, heart rate, and the presence of chest pain.
  
  Our second goal is to create models for the purpose of interpretation, which can be used to provide a greater understanding of signs that at-risk patients can analyze to check their risk for CVDs. 
  
  We chose to fit 3 different models to classify whether a patient has heart disease or not. The first model is a logistic regression model with variable selection performed by a lasso regression. We decided to use this model because of its interpretability so that we can examine the relationship our predictors have with the probability of a patient having heart disease. We also decided to use a random forest and a SVM because of their ability to perform classification and due to their predictive power despite their lack of interpretability. We will use a 10-fold CV to determine the best predictive model based on the classification error of each model. Due to the ability of ensemble models to reduce prediction error, we will also create an ensemble model where the prediction is the most common result of the 3 individual models (https://www.sciencedirect.com/topics/computer-science/ensemble-modeling ). Finally, we compare the 10-fold CV errors for each individual model as well as the ensemble to find the one with lowest classification error and therefore highest predictive accuracy. 

## II. Data
  We obtained our data from Kaggle (https://www.kaggle.com/fedesoriano/heart-failure-prediction). The dataset was originally provided by Dr. David Aha, a researcher at the US Naval Research Laboratory. It was created by combining five heart datasets from Cleveland (303 observations), Hungary (294 observations), Switzerland (123 observations), Long Beach, Virginia (200 observations), and Stalog (270 observations), for 918 unique observations. This makes it one of the largest available heart disease datasets. The dataset has 11 predictor variables, which are age, sex, chest pain types, resting BP, cholesterol, fasting blood sugar, resting ECG results, maximum heart rate, exercise-induced angina, oldpeak, and slope of the peak exercise ST segment. Sex, chest pain types, resting ECG, exercise-induced angina, and ST slope are categorical variables, while the rest are numeric. We made sure to factor the categorical variables in order to make sure R would treat them as categorical. For our purposes, the dataset has one response variable, which is whether or not the patient has heart disease. Because in the dataset the frequency of heart disease is roughly equal to the frequency of non-heart disease, the data is likely not based on the general population, where we would expect the frequency of heart disease to be a lot lower. As a result, the inference and predictions from our models do not apply to the general population, but instead apply to the population this dataset was drawn from, which is patients who are at risk of heart disease. 
  
There were a total of 170 observations with missing values in Cholesterol and Resting Blood Pressure(BP). (Cholesterol value of 0 and Resting Blood Pressure value of 0 are considered missing since these are impossible to reach in real life). The visualizations of the missing values in [Table 00] shows that there are 170 observations with missing Cholesterol value and 1 observation with missing Resting BP value. For the one observation with missing Resting BP, it also had missing Cholesterol value. This observation was disregarded for analysis under Missing Completely at Random (MCAR) assumption. For the 169 observations with missing Cholesterol values, imputations were conducted based on Missing at Random (MAR) assumption. Multiple Imputation using Chained Equation (MICE) in R was used to impute the values. Five chains of imputations were conducted for Cholesterol with each chain using the default method of predictive mean matching (ppm) method since Cholesterol was a numerical variable.

| Variables      | Description                                                                                                                                                                                                                                | Type |   |   |
|----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------|---|---|
| Age            | Age of the patient in years                                                                                                                                                                                                                |      |   |   |
| Sex            | Sex of the patient [M/F]                                                                                                                                                                                                                   |      |   |   |
| ChestPainType  | Chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]                                                                                                                                       |      |   |   |
| RestingBP      | Resting blood pressure [mm Hg]                                                                                                                                                                                                             |      |   |   |
| Cholesterol    | Serum cholesterol [mm/dl]                                                                                                                                                                                                                  |      |   |   |
| FastingBS      | Fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]                                                                                                                                                                            |      |   |   |
| RestingECG     | Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria] |      |   |   |
| MaxHR          | Maximum heart rate achieved [Numeric value between 60 and 202]                                                                                                                                                                             |      |   |   |
| ExerciseAngina | Exercise-induced angina [Y: Yes, N: No]                                                                                                                                                                                                    |      |   |   |
| Oldpeak        | [Numeric value measured in depression]                                                                                                                                                                                                     |      |   |   |
| HeartDisease   | Output class [1: heart disease, 0: Normal]                                                                                                                                                                                                 |      |   |   |

## III. Methodology 

Our first model is a logistic regression model. The logistic regression model is formulated by the equation:
$$\log(\frac{P}{1-P}) = \beta_0+\beta_1X_1+\beta_2X_2+...+\beta_kX_k$$
Using logistic regression lends itself well to inference and classification goals, and using the Lasso method will perform model selection. Therefore, we decided to use a logistic lasso regression. This is a logistic regression with shrinkage applied to the coefficients of the logistic regression. The amount of shrinkage is controlled by a shrinkage parameter lambda, and in lasso shrinkage, the coefficients are able to be shrunk to 0, which means that lasso can perform variable selection. We used the glmnet package to fit the lasso regression model. We used cross-validation to determine the lambda value that results in the lowest CV error, and then fitted the lasso regression model with the selected lambda value. We then used any active main effects, and interaction effects with non-zero values for any level to fit the final logistic regression using the glm() function. Even though many of the main effects for the logistic regression were not active (had coefficient values of 0), we made sure to include any main effects associated with the active interaction effects due to the hierarchy principle. 

Our second model uses a random forest. With random forests, we fit a large number of binary decision trees, each on a bagged set of data and each with a random subset of predictors, then average the trees to get a final prediction. By using random subsets of predictors, we decorrelate the trees and reduce the variance compared to other methods like bagging. The random forest method yields itself well to the project goals at hand because of the categorical and binary nature of the outcomes we’re trying to predict. We built a random forest of 1000 trees built off of bootstrapped Heart Data. We specified number of predictors sampled for spitting at each node as sqrt(11) ~ 3. We chose sqrt(11) since this is the industry-standard for classification. We chose 1000 trees in order to lower our type 2 error, as we can see in our error plot. 

Finally, our third model uses a Support Vector Machine (SVM). An SVM uses kernels to provide a flexible classification model. It separates the domain of the data with the goal maximizing the margin distance while allowing for a small number of misclassified training points. Although SVMs are not very interpretable and therefore wouldn’t fit our inference goals, they tend to yield high prediction accuracy and work well with categorical response variables, so we fit an SVM for prediction purposes to compare with our other models. The cost tuning parameters for our SVM models are determined using cross validation, and the best kernel is determined at the end based on the misclassification error from a 70-30 training-test split. The best kernel ended up being a tie between the linear and radial kernals, so we chose the linear kernel for parity. The equation for the linear kernel is:

$$K(x_i,x_{i'})=(1+\sum_{j=1}^{p}x_{ij}x_{i'j})^d$$


## IV. Results
## V. Conclusion

